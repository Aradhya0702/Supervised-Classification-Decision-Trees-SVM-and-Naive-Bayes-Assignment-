{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "   - Information Gain is a measure used in decision tree algorithms to determine the effectiveness of a feature (attribute) in classifying the training data. It quantifies how much the uncertainty (entropy) of the target variable decreases when you split a dataset based on a particular feature. In simpler terms, it tells you how much 'information' a feature provides about the class labels.\n",
        "\n",
        "Key Concepts:\n",
        "\n",
        " 1. Entropy: This is a measure of the impurity or disorder in a set of examples. If a dataset is perfectly homogeneous (all examples belong to the same class), its entropy is zero. If the dataset is evenly split between multiple classes, its entropy is high. The formula for entropy H(S) for a set S with c classes is:\n",
        "\n",
        "H(S) = - Σ (p_i * log2(p_i))\n",
        "\n",
        "where p_i is the proportion of examples belonging to class i in set S.\n",
        "\n",
        "  2. Conditional Entropy: This measures the entropy of the target variable after splitting the dataset by a particular feature. It's the weighted average of the entropy of each subset created by the split.\n",
        "\n",
        "  - How is it used in Decision Trees?\n",
        "\n",
        "Decision Tree algorithms aim to build a tree structure that best classifies instances by making a series of decisions. At each step of building the tree, the algorithm needs to decide which feature to split on at a given node. This is where Information Gain comes in:\n",
        "\n",
        " 1. Feature Selection: The algorithm calculates the Information Gain for each available feature at a particular node.\n",
        " 2. Optimal Split: The feature with the highest Information Gain is chosen as the splitting criterion for that node. A high Information Gain means that splitting on this feature will reduce the uncertainty or entropy of the target variable the most, leading to more homogeneous child nodes.\n",
        " 3. Recursive Process: This process is applied recursively to each child node until a stopping condition is met (e.g., all instances in a node belong to the same class, no more features to split on, or a maximum tree depth is reached).\n",
        "In essence, Information Gain helps the decision tree to find the most informative features that allow it to make the best possible classification decisions at each step, leading to a more accurate and efficient tree.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qy4WsrM5w1rb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "   - Both Gini Impurity and Entropy are measures used in decision tree algorithms to quantify the impurity or disorder of a set of data, helping determine the best split at each node. While they serve the same fundamental purpose, they differ in their mathematical formulation and practical implications:\n",
        "\n",
        "1. Gini Impurity:\n",
        "\n",
        "  - Definition: Gini Impurity measures how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset.\n",
        "\n",
        "  - Formula: For a dataset S with c classes, the Gini Impurity G(S) is calculated as:\n",
        "\n",
        "G(S) = 1 - Σ (p_i)^2\n",
        "\n",
        "where p_i is the proportion of elements belonging to class i in set S.\n",
        "\n",
        "  - Range: Its value ranges from 0 (pure, all elements belong to the same class) to 0.5 (maximum impurity for a binary classification, where classes are equally distributed).\n",
        "\n",
        "  - Bias: It tends to isolate the most frequent class in its own branch of the tree.\n",
        "\n",
        "  - Computation: Gini Impurity involves squared probabilities, making it computationally faster as it avoids logarithmic calculations.\n",
        "\n",
        "2. Entropy:\n",
        "\n",
        "  - Definition: Entropy measures the average amount of information needed to identify the class of an element in a set. It quantifies the level of disorder or unpredictability.\n",
        "\n",
        "  - Formula: For a dataset S with c classes, the Entropy H(S) is calculated as:\n",
        "\n",
        "H(S) = - Σ (p_i * log2(p_i))\n",
        "\n",
        "where p_i is the proportion of elements belonging to class i in set S.\n",
        "\n",
        "  - Range: Its value ranges from 0 (pure, all elements belong to the same class) to 1 (maximum impurity for a binary classification, where classes are equally distributed).\n",
        "\n",
        "  - Bias: It tends to produce more balanced trees, splitting on attributes that result in child nodes with a more uniform distribution of classes.\n",
        "\n",
        "  - Computation: Entropy involves logarithmic calculations, which can be slightly more computationally intensive than Gini Impurity.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ih84HHxo1mQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "   - Pre-pruning is a technique used in decision trees to prevent overfitting by stopping the tree's growth early. Instead of building a full tree and then cutting it back (post-pruning), pre-pruning sets conditions during construction to decide if a node should split further.\n",
        "\n",
        "Common pre-pruning criteria include:\n",
        "\n",
        "1. Maximum Depth: Limiting how deep the tree can grow.\n",
        "2. Minimum Samples for a Split: Requiring a minimum number of data points in a node before it can be split.\n",
        "3. Minimum Samples in a Leaf Node: Ensuring that any resulting leaf nodes have at least a minimum number of samples.\n",
        "4. Impurity Threshold / Information Gain Threshold: Stopping splits if the resulting improvement in purity (information gain) is not significant enough.\n",
        "5. Advantages: It reduces overfitting, speeds up training, and often results in simpler, more interpretable models.\n",
        "\n",
        "6. Disadvantages: It can sometimes stop the tree too early, missing potentially beneficial splits down the line, which might lead to an underfitted model.\n",
        "\n"
      ],
      "metadata": {
        "id": "5WxGaQWS2fT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "\n"
      ],
      "metadata": {
        "id": "c7M70e933KZE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "75449714",
        "outputId": "f959d4df-59c6-4059-c61c-986658c48fae"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=2, n_redundant=0, random_state=42)\n",
        "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "print(\"Sample of the generated dataset:\")\n",
        "display(df.head())\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of the generated dataset:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   feature_0  feature_1  feature_2  feature_3  feature_4  target\n",
              "0  -0.790882   0.957840   0.723074   0.290947   0.379427       1\n",
              "1   1.372848  -0.180392  -1.067639  -0.357445  -1.055437       0\n",
              "2   0.283751   0.695203   1.310075   1.370536   0.539354       1\n",
              "3  -0.610858   1.355573  -1.290046  -1.570881   1.348138       0\n",
              "4   0.307677   0.207603  -0.870961   0.957792   1.197704       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-162ec200-2e27-4fc6-be5f-70c178f4f65d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_0</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.790882</td>\n",
              "      <td>0.957840</td>\n",
              "      <td>0.723074</td>\n",
              "      <td>0.290947</td>\n",
              "      <td>0.379427</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.372848</td>\n",
              "      <td>-0.180392</td>\n",
              "      <td>-1.067639</td>\n",
              "      <td>-0.357445</td>\n",
              "      <td>-1.055437</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.283751</td>\n",
              "      <td>0.695203</td>\n",
              "      <td>1.310075</td>\n",
              "      <td>1.370536</td>\n",
              "      <td>0.539354</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.610858</td>\n",
              "      <td>1.355573</td>\n",
              "      <td>-1.290046</td>\n",
              "      <td>-1.570881</td>\n",
              "      <td>1.348138</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.307677</td>\n",
              "      <td>0.207603</td>\n",
              "      <td>-0.870961</td>\n",
              "      <td>0.957792</td>\n",
              "      <td>1.197704</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-162ec200-2e27-4fc6-be5f-70c178f4f65d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-162ec200-2e27-4fc6-be5f-70c178f4f65d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-162ec200-2e27-4fc6-be5f-70c178f4f65d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1f8ca510-084b-4a7e-93a9-e7b27deb2277\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1f8ca510-084b-4a7e-93a9-e7b27deb2277')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1f8ca510-084b-4a7e-93a9-e7b27deb2277 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"feature_0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8653386697006828,\n        \"min\": -0.7908817916945654,\n        \"max\": 1.3728484746765302,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.3728484746765302,\n          0.30767665340111383,\n          0.2837510236615011\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6063688739141166,\n        \"min\": -0.18039168176240783,\n        \"max\": 1.355572935710348,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -0.18039168176240783,\n          0.20760335140062577,\n          0.6952029000839993\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1743014606775473,\n        \"min\": -1.290045531094827,\n        \"max\": 1.3100751863130045,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -1.0676394487710172,\n          -0.8709609446138056,\n          1.3100751863130045\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1597778187368961,\n        \"min\": -1.5708814571259122,\n        \"max\": 1.3705361439967134,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -0.3574454228104272,\n          0.9577916144729056,\n          1.3705361439967134\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.953932721670546,\n        \"min\": -1.0554373616464363,\n        \"max\": 1.3481375214567366,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -1.0554373616464363,\n          1.1977038971988485,\n          0.5393541147194907\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5caa4258"
      },
      "source": [
        "### Training a Decision Tree Classifier with Gini Impurity\n",
        "\n",
        "Now, let's train the `DecisionTreeClassifier` using `criterion='gini'` as specified. After training, we will access the `.feature_importances_` attribute to see how much each feature contributed to the model's decision-making process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fe9ecdd",
        "outputId": "2e5a83cc-9a43-46c9-83bb-27d10b93c0fd"
      },
      "source": [
        "# 2. Initialize and train the Decision Tree Classifier\n",
        "# Use criterion='gini' for Gini Impurity\n",
        "dtree_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dtree_gini.fit(X_train, y_train)\n",
        "\n",
        "# 3. Print feature importances\n",
        "print(\"\\nFeature Importances (using Gini Impurity):\\n\")\n",
        "for feature, importance in zip(feature_names, dtree_gini.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "# Optionally, you can also print the accuracy on the test set\n",
        "accuracy = dtree_gini.score(X_test, y_test)\n",
        "print(f\"\\nModel Accuracy on Test Set: {accuracy:.4f}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature Importances (using Gini Impurity):\n",
            "\n",
            "feature_0: 0.0339\n",
            "feature_1: 0.0517\n",
            "feature_2: 0.6962\n",
            "feature_3: 0.0250\n",
            "feature_4: 0.1932\n",
            "\n",
            "Model Accuracy on Test Set: 0.8567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "  - A Support Vector Machine (SVM) is a powerful machine learning algorithm primarily used for classification (and regression). Its main goal is to find the \"best\" separating hyperplane (a decision boundary) that maximizes the margin between different classes in a high-dimensional space.\n",
        "\n",
        "Key aspects:\n",
        "\n",
        " 1. Hyperplane & Margin: The SVM aims for a hyperplane that separates classes with the largest possible distance (margin) to the nearest data points (called Support Vectors).\n",
        " 2. Support Vectors: These are the critical data points closest to the hyperplane that define its position and the margin.\n",
        " 3. Kernel Trick: For data that isn't linearly separable, SVMs use a \"kernel trick\" to implicitly map data into a higher-dimensional space where a linear separation might be possible. Common kernels include RBF, Polynomial, and Sigmoid.\n",
        " 4. Soft Margin: To handle noisy data or overlapping classes, SVMs allow for some misclassifications or points within the margin, controlled by a regularization parameter (C-parameter).\n",
        "In essence, SVMs are effective classifiers that seek an optimal, robust separation boundary, even for complex, non-linear data through dimensional transformation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "myNMr76Z3-sF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "  - The Kernel Trick is a powerful technique used in Support Vector Machines (SVMs) to handle non-linearly separable data without explicitly transforming the data into a higher-dimensional space.\n",
        "\n",
        "Here's the essence:\n",
        "\n",
        " 1. Problem: Some datasets cannot be separated by a straight line (hyperplane) in their original feature space.\n",
        " 2. Solution (Conceptual): If we could map this data into a much higher-dimensional space, it might become linearly separable there.\n",
        " 3. Kernel Trick: Instead of actually performing this computationally expensive, explicit mapping, the kernel trick uses a kernel function (e.g., RBF, polynomial) that calculates the dot product of the data points as if they were already in that higher-dimensional space. This allows SVMs to find a non-linear decision boundary in the original space using linear separation in a transformed, implicit space.\n",
        "In short, it allows SVMs to classify complex, non-linear patterns efficiently by operating in a 'feature space' that is never explicitly computed.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yb9R2r6i4ez_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "-\n",
        "\n"
      ],
      "metadata": {
        "id": "EbpDKClD49Ry"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98858f40",
        "outputId": "6525e7ff-f0b5-4f0e-9118-b778c74d2db6"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Get feature names for better understanding (optional, but good practice)\n",
        "feature_names = wine.feature_names\n",
        "\n",
        "print(\"Wine dataset loaded successfully. Shape of X: \", X.shape)\n",
        "print(\"Number of classes: \", len(wine.target_names))\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "# Using stratify=y to ensure that both training and test sets have\n",
        "# a similar proportion of examples in each class as the full dataset.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wine dataset loaded successfully. Shape of X:  (178, 13)\n",
            "Number of classes:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ae8ba2a"
      },
      "source": [
        "### Training SVM Classifiers and Comparing Accuracies\n",
        "\n",
        "Now, we'll train two `SVC` models: one using a `linear` kernel and another using an `rbf` (Radial Basis Function) kernel. After training, we will evaluate each model's accuracy on the test set and display the comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9c36e36",
        "outputId": "d1845e3f-2f1e-4c9f-c99d-e2739f2ffc39"
      },
      "source": [
        "# 3. Train SVM with Linear Kernel\n",
        "# kernel='linear' specifies a linear SVM\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions and evaluate for Linear Kernel\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "print(f\"Accuracy of SVM with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "\n",
        "# 5. Train SVM with RBF Kernel\n",
        "# kernel='rbf' specifies a Radial Basis Function kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# 6. Make predictions and evaluate for RBF Kernel\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "print(f\"Accuracy of SVM with RBF Kernel: {accuracy_rbf:.4f}\")\n",
        "\n",
        "print(\"\\n--- Comparison ---\")\n",
        "if accuracy_linear > accuracy_rbf:\n",
        "    print(f\"The Linear Kernel SVM performed better with an accuracy of {accuracy_linear:.4f}.\")\n",
        "elif accuracy_rbf > accuracy_linear:\n",
        "    print(f\"The RBF Kernel SVM performed better with an accuracy of {accuracy_rbf:.4f}.\")\n",
        "else:\n",
        "    print(f\"Both Linear and RBF Kernel SVMs performed equally with an accuracy of {accuracy_linear:.4f}.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear Kernel: 0.9444\n",
            "Accuracy of SVM with RBF Kernel: 0.6667\n",
            "\n",
            "--- Comparison ---\n",
            "The Linear Kernel SVM performed better with an accuracy of 0.9444.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "   - The Naïve Bayes classifier is a simple probabilistic classification algorithm based on Bayes' Theorem. It's commonly used for tasks like text classification (e.g., spam detection) due to its efficiency.\n",
        "\n",
        " - It's called \"Naïve\" because it makes a strong, often unrealistic, assumption: all features are independent of each other, given the class. For example, if an email has 'Viagra' and 'cheap pharmacy', Naïve Bayes assumes these words appear independently, even if in reality they are correlated.\n",
        "\n",
        "Despite this \"naïve\" assumption, it performs surprisingly well in many real-world scenarios, especially when computations need to be fast and efficient.\n",
        "\n"
      ],
      "metadata": {
        "id": "OcgbOjkR5yQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "  - The three main types of Naïve Bayes classifiers are distinguished by the assumptions they make about the distribution of their features:\n",
        "\n",
        " - Gaussian Naïve Bayes:\n",
        "\n",
        " 1. Feature Type: Continuous (numerical data, like height or weight).\n",
        " 2. Assumption: Features follow a Gaussian (normal) distribution.\n",
        " 3. Use Case: Ideal for numerical datasets.\n",
        " - Multinomial Naïve Bayes:\n",
        "\n",
        " 1. Feature Type: Discrete counts (e.g., word frequencies).\n",
        " 2. Assumption: Features represent the frequency of events.\n",
        " 3. Use Case: Primarily for text classification where word counts are important.\n",
        " - Bernoulli Naïve Bayes:\n",
        "\n",
        " 1. Feature Type: Binary (presence or absence of a feature).\n",
        " 2. Assumption: Features are Boolean, indicating if something exists or not.\n",
        " 3. Use Case: Also for text classification, focusing on whether a word is present, not how many times.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ki9WqBUj6V4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k0RhvaAL7Siy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44fc7bda",
        "outputId": "85e52288-5029-4de0-ee1a-af2d33185003"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "print(\"Breast Cancer dataset loaded successfully. Shape of X: \", X.shape)\n",
        "print(\"Number of features: \", breast_cancer.feature_names.shape[0])\n",
        "print(\"Number of classes: \", len(breast_cancer.target_names))\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Breast Cancer dataset loaded successfully. Shape of X:  (569, 30)\n",
            "Number of features:  30\n",
            "Number of classes:  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fcb4be7"
      },
      "source": [
        "### Training a Gaussian Naïve Bayes Classifier\n",
        "\n",
        "Now, we'll initialize and train the `GaussianNB` classifier. After training, we will use it to make predictions on the test set and then calculate the accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f405a6a",
        "outputId": "2b151da6-cd66-4f8b-b7a0-6ac3ae24783e"
      },
      "source": [
        "# 3. Initialize the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# 4. Train the classifier on the training data\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 6. Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nGaussian Naïve Bayes Classifier Accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gaussian Naïve Bayes Classifier Accuracy: 0.9415\n"
          ]
        }
      ]
    }
  ]
}